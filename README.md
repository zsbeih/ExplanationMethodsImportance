# Importance of Explanation Methods

This project implements a deep learning pipeline using a ResNet-18 model trained on the CIFAR-10 dataset. It uses the Captum library to generate visual explanations for model predictions using attribution methods such as Integrated Gradients, Saliency, and Shapley Value Sampling. The project includes functionality for training, testing, and visualizing attribution maps to interpret the importance of input features and the differences between the methodologies of these attribution methods.
